import org.apache.spark.storage.StorageLevel
import org.apache.spark.storage.StorageLevel
 
val lines = sc.textFile("hdfs:///user/raj/data.txt", 3)
//lines: org.apache.spark.rdd.RDD[String] = hdfs:///user/raj/data.txt MapPartitionsRDD[1] at textFile at <console>:28
 
 
//lines.partitions.size
// flatMap() : One of many transformation
 
val words = lines.flatMap(x => x.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:30
 
// Persist the data
 
val units = words.map ( word => (word, 1) ).persist(StorageLevel.MEMORY_ONLY)
//units: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:32
 
val counts = units.reduceByKey ( (x, y) => x + y )
//counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:34
 
// Text file is read to compute the 'counts' RDD
counts.toDebugString
 
// First Action
counts.collect()
//res2: Array[(String, Int)] = Array((another,1), (This,2), (is,2), (a,1), (test,2))
 
val counts2 = units.reduceByKey((x, y) => x * y)
//counts2: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:34
 
// Cache value is read to compute the 'counts2' RDD
counts2.toDebugString
 
// Second Action
 
counts2.collect()
//res4: Array[(String, Int)] = Array((another,1), (This,1), (is,1), (a,1), (test,1))
