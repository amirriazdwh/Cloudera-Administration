df.write.format("com.databricks.spark.csv").save("/user/amir")
// works
d.write.format("com.databricks.spark.csv").option("header", "true").save("/user/hive/warehouse/newcars.csv")
df.write.format("com.databricks.spark.csv").option("header", "true").save("file.csv")
// this one works
d.write.format("json").save("/user/hive/warehouse/test")
 textDataDF.write.format("parquet").save("db_bdp.textData")

d.write.format("com.databricks.spark.xml").saveAsTable("db_bdp.textData")
d.write.format("parquet").save("db_bdp.textData")

spark-shell --driver-memory 512m --executor-memory 512m --executor-cores 2

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val d =sqlContext.read.json("/user/hive/warehouse/test")
sqlContext.read.parquet("/user/hive/warehouse/testparquet").registerTempTable("First")
sqlContext.read.json("/user/hive/warehouse/test").registerTempTable("First")
val d =sqlContext.sql("select * from First")
val d =sqlContext.sql("select * from products")

d.write.format("json").save("/user/hive/warehouse/test")
d.write.format("parquet").save("/user/hive/warehouse/xmltoparqute")

.registerTempTable("First")


//show temporary table in context
sqlContext.tables

val peopleDF = sqlContext.read.format("com.databricks.spark.xml").load("/user/hive/warehouse/books.xml").registerTempTable("First")

val peopleDF = sqlContext.read.format("csv").load("/user/hive/warehouse/newcars.csv").registerTempTable("First")
val peopleDF = sqlContext.read.format("json").load("/user/hive/warehouse/testparquet").registerTempTable("First")

val peopleDF = sqlContext.read.format("parquet").load("/user/hive/warehouse/xmltoparqute").registerTempTable("First")
peopleDF.select("category_id", "category_name").write.format("parquet").save("cidAndcname.parquet")


/
val prop = new java.util.Properties
prop.setProperty("driver", "com.mysql.jdbc.Driver")
prop.setProperty("user", "vaquar")
prop.setProperty("password", "khan") 

//jdbc mysql url - destination database is named "temp"
val url = "jdbc:mysql://localhost:3306/temp"

//destination database table 
val table = "sample_data_table"

//write data from spark dataframe to database
df.write.mode("append").jdbc(url, dbtable, prop)

./spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0 --class com.inndata.StructuredStreaming.Kafka --master local[*] /Users/apple/.m2/repository/com/inndata/StructuredStreaming/0.0.1SNAPSHOT/StructuredStreaming-0.0.1-SNAPSHOT.jar


spark-shell --jars jarcvs/spark-csv_2.10-1.5.0.jar,jarcvs/commons-csv-1.1.jar

https://repo1.maven.org/maven2/com/databricks/spark-xml/0.5.0/

spark-shell --packages com.databricks:spark-csv_2.11:1.4.0 --driver-memory 4g --executor-memory 4g --driver-java-options "-XX:MaxPermSize=1024m"

$SPARK_HOME/bin/spark-shell 

spark-shell --jars=spark-xml_2.10-0.4.1.jar
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext.read.format("com.databricks.spark.xml").option("rootTag", "books").option("rowTag", "book").load("/user/hive/warehouse/books.xml").registerTempTable("First")
val d =sqlContext.sql("select * from First")

val df = sqlContext.read.option("rowTag", "book").xml("/user/hive/warehouse/books.xml)

EATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string) USING com.databricks.spark.xml
OPTIONS (path "books.xml", rowTag "book")

sqlContext.sql("CREATE TABLE books USING com.databricks.spark.xml OPTIONS (path """books.xml""", rowTag """book:""")")

       sqlContext.sql("CREATE TABLE books USING com.databricks.spark.xml OPTIONS (path "/user/hive/warehouse/books.xml", rowTag "book")")

------------------------
spark listener testing
=========================
sqlContext.read.parquet("/user/hive/warehouse/testparquet").registerTempTable("First")

