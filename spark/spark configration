    val schema = StructType(columns
      .map(fieldName => StructField(fieldName, StringType, nullable = true)))


val data = Array(Row("Java", "20000"), Row("Python", "100000"), Row("Scala", "3000"))
    val dfFromRDD3 = spark.createDataFrame(data,schema)

    fs.getFileStatus(new Path("/home/cloudera/spark/")).getPath().toString

spark://192.168.56.103/:7077

SparkConf conf = new SparkConf()
            .setAppName("Test spark")
            .setMaster("spark://ip of your master node:port of your master node")
            .set("spark.blockManager.port", "10025")
            .set("spark.driver.blockManager.port", "10026")
            .set("spark.driver.port", "10027") //make all communication ports static (not necessary if you disabled firewalls, or if your nodes located in local network, otherwise you must open this ports in firewall settings)
            .set("spark.cores.max", "12") 
            .set("spark.executor.memory", "2g")
            .set("spark.driver.host", "ip of your driver (PC)"); //(necessary)


org.apache.spark.deploy.SparkSubmit --conf spark.executor.memory=8g 
--conf spark.yarn.am.memory=4096M --conf spark.master=yarn --conf spark.driver.memory=4g 
--conf spark.rpc.askTimeout=600s --conf spark.yarn.am.cores=2 
--conf spark.network.timeout=600s --conf spark.executor.cores=1 
--conf spark.app.name=oss_pyspark_tb_mk_nl_s1u_http_status_cell_hour_IP*.*_PID22012_20190506_090035 
--conf spark.speculation=true --conf spark.dynamicAllocation.maxExecutors=100 
--conf spark.sql.shuffle.partitions=100 
--conf spark.sql.small.file.split.size=2147483646 
--conf spark.kryo.registrationRequired=false
 --conf spark.default.parallelism=400 
 --conf spark.sql.autoBroadcastJoinThreshold=104857600 
 --conf spark.shuffle.service.enabled=true --conf spark.executor.instances=60 --conf spark.sql.small.file.combine=false --conf spark.yarn.queue=hwcdm --conf spark.sql.sources.maxConcurrentWrites=50 --conf spark.dynamicAllocation.enabled=true pyspark-shell