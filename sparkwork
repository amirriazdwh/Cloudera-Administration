df.write.format("com.databricks.spark.csv").save("/user/amir")

d.write.format("com.databricks.spark.csv").option("header", "true").save("/user/hive/warehouse/newcars.csv")
df.write.format("com.databricks.spark.csv").option("header", "true").save("file.csv")
// this one works
d.write.format("json").save("/user/hive/warehouse/test")


spark-shell --driver-memory 512m --executor-memory 512m --executor-cores 2

val sqlContext = new org.apache.spark.sql.SQLContext(sc)
val d =sqlContext.read.json("/user/hive/warehouse/test")
sqlContext.read.parquet("/user/hive/warehouse/testparquet").registerTempTable("First")
sqlContext.read.json("/user/hive/warehouse/test").registerTempTable("First")
val d =sqlContext.sql("select * from First")
val d =sqlContext.sql("select * from products")

d.write.format("json").save("/user/hive/warehouse/test")
d.write.format("parquet").save("/user/hive/warehouse/testparquet").registerTempTable("First")


//show temporary table in context
sqlContext.tables

val peopleDF = sqlContext.read.format("json").load("/user/hive/warehouse/testparquet").registerTempTable("First")

val peopleDF = sqlContext.read.format("parquet").load("/user/hive/warehouse/testparquet")
peopleDF.select("category_id", "category_name").write.format("parquet").save("cidAndcname.parquet")


/
val prop = new java.util.Properties
prop.setProperty("driver", "com.mysql.jdbc.Driver")
prop.setProperty("user", "vaquar")
prop.setProperty("password", "khan") 

//jdbc mysql url - destination database is named "temp"
val url = "jdbc:mysql://localhost:3306/temp"

//destination database table 
val table = "sample_data_table"

//write data from spark dataframe to database
df.write.mode("append").jdbc(url, dbtable, prop)



spark-shell --jars jarcvs/spark-csv_2.10-1.5.0.jar,jarcvs/commons-csv-1.1.jar



